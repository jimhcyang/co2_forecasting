{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe2dd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nco2_pilot_pipeline.py\\n\\nPilot pipeline for CO2 markets:\\n  - Load 5 carbon markets from Excel (Refinitiv exports).\\n  - Slice to 2022-01-01 ... 2024-12-31.\\n  - Compute price-only technical indicators.\\n  - Fetch macro panel from Yahoo Finance (indices, commodities, FX).\\n  - Correlation-based feature selection (top-k tech + macro).\\n  - 10-day rolling windows, 90% train / 10% test.\\n  - Models: naive baseline, XGBoost, Keras MLP, Keras GRU.\\n\\nDesigned as a proof of concept; you can later:\\n  - Swap macro tickers per region,\\n  - Beef up feature selection,\\n  - Plug into your existing logging / tuning infra.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "co2_pilot_pipeline.py\n",
    "\n",
    "Pilot pipeline for CO2 markets:\n",
    "  - Load 5 carbon markets from Excel (Refinitiv exports).\n",
    "  - Slice to 2022-01-01 ... 2024-12-31.\n",
    "  - Compute price-only technical indicators.\n",
    "  - Fetch macro panel from Yahoo Finance (indices, commodities, FX).\n",
    "  - Correlation-based feature selection (top-k tech + macro).\n",
    "  - 10-day rolling windows, 90% train / 10% test.\n",
    "  - Models: naive baseline, XGBoost, Keras MLP, Keras GRU.\n",
    "\n",
    "Designed as a proof of concept; you can later:\n",
    "  - Swap macro tickers per region,\n",
    "  - Beef up feature selection,\n",
    "  - Plug into your existing logging / tuning infra.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4e1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4462240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "CO2_FILES = {\n",
    "    \"Shanghai\": \"Shanghai Spot - 2020.8.1 - 2025.11.28.xlsx\",\n",
    "    \"Beijing\": \"Beijing - 2020.8.1 - 2025.11.28.xlsx\",\n",
    "    \"Japan\": \"Japan - 01-Oct-2023 - 28-Nov-2025.xlsx\",\n",
    "    \"EUA_ICE\": \"EUA - 01-Sep-2020 - 24-Nov-2025.xlsx\",\n",
    "    \"EUA_EEX\": \"EEX-EUSP4-SPOT - 01Jan2021 - 28Nov 2025.xlsx\",\n",
    "}\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# project time slice\n",
    "START_DATE = \"2022-01-01\"\n",
    "END_DATE   = \"2024-12-31\"\n",
    "\n",
    "# macro proxies (can refine per-region later)\n",
    "MACRO_TICKERS = [\n",
    "    # Global / US risk\n",
    "    \"^GSPC\", \"^IXIC\",\n",
    "    # China\n",
    "    \"000001.SS\", \"399001.SZ\",\n",
    "    # Japan\n",
    "    \"^N225\",\n",
    "    # Euro area (Euro Stoxx 50)\n",
    "    \"^STOXX50E\",\n",
    "    # Energy\n",
    "    \"CL=F\", \"BZ=F\", \"NG=F\",\n",
    "    # Metals\n",
    "    \"GC=F\", \"SI=F\", \"HG=F\",\n",
    "    # Dollar index\n",
    "    \"DX-Y.NYB\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default Keras hyperparams (single config each, as requested)\n",
    "DEFAULT_MLP_CONFIG = {\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"hidden_units\": (64, 32),\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "DEFAULT_GRU_CONFIG = {\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"hidden_units\": 32,\n",
    "    \"dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f2b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# TECHNICAL INDICATORS (price-only)\n",
    "#   Reusing the style of your stock pipeline, but only using close/price.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def SMA(series: pd.Series, window: int = 20) -> pd.Series:\n",
    "    return series.rolling(window=window, min_periods=window).mean()\n",
    "\n",
    "def MACD(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "    fast_ema = series.ewm(span=fast, adjust=False, min_periods=fast).mean()\n",
    "    slow_ema = series.ewm(span=slow, adjust=False, min_periods=slow).mean()\n",
    "    macd_line = fast_ema - slow_ema\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False, min_periods=signal).mean()\n",
    "    hist = macd_line - signal_line\n",
    "    return macd_line, signal_line, hist\n",
    "\n",
    "def RSI(series: pd.Series, window: int = 14) -> pd.Series:\n",
    "    delta = series.diff()\n",
    "    gains = delta.where(delta > 0, 0.0)\n",
    "    losses = -delta.where(delta < 0, 0.0)\n",
    "    avg_gain = gains.ewm(alpha=1/window, adjust=False, min_periods=window).mean()\n",
    "    avg_loss = losses.ewm(alpha=1/window, adjust=False, min_periods=window).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def BBANDS(series: pd.Series, window: int = 20, num_std: float = 2.0):\n",
    "    mid = series.rolling(window=window, min_periods=window).mean()\n",
    "    rolling_std = series.rolling(window=window, min_periods=window).std()\n",
    "    upper = mid + num_std * rolling_std\n",
    "    lower = mid - num_std * rolling_std\n",
    "    return mid, upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e882e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_technical_features(price: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Price-only indicators that make sense for CO2 markets:\n",
    "      - 1d / 5d returns, 20d volatility\n",
    "      - Short/medium/long SMAs + slope/spread\n",
    "      - 20d z-score (distance from local mean)\n",
    "      - Momentum and ROC\n",
    "      - RSI(14) for overbought/oversold\n",
    "      - MACD (trend) + Bollinger bands\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=price.index)\n",
    "\n",
    "    ret_1d = price.pct_change()\n",
    "    log_ret_1d = np.log(price).diff()\n",
    "\n",
    "    df[\"ret_1d\"] = ret_1d\n",
    "    df[\"log_ret_1d\"] = log_ret_1d\n",
    "    df[\"ret_5d\"] = price.pct_change(5)\n",
    "    df[\"vol_20d\"] = ret_1d.rolling(20).std()\n",
    "\n",
    "    df[\"sma_5\"] = SMA(price, 5)\n",
    "    df[\"sma_20\"] = SMA(price, 20)\n",
    "    df[\"sma_60\"] = SMA(price, 60)\n",
    "    df[\"sma_5_20_diff\"] = df[\"sma_5\"] - df[\"sma_20\"]\n",
    "\n",
    "    roll_mean_20 = price.rolling(20).mean()\n",
    "    roll_std_20 = price.rolling(20).std()\n",
    "    df[\"zscore_20\"] = (price - roll_mean_20) / roll_std_20\n",
    "\n",
    "    df[\"momentum_10\"] = price.diff(10)\n",
    "    df[\"roc_10\"] = price.pct_change(10)\n",
    "\n",
    "    df[\"rsi_14\"] = RSI(price, 14)\n",
    "\n",
    "    macd_line, macd_signal, macd_hist = MACD(price, 12, 26, 9)\n",
    "    df[\"macd_line\"] = macd_line\n",
    "    df[\"macd_signal\"] = macd_signal\n",
    "    df[\"macd_hist\"] = macd_hist\n",
    "\n",
    "    _, bb_up, bb_low = BBANDS(price, 20, 2.0)\n",
    "    df[\"bb_upper_20_2\"] = bb_up\n",
    "    df[\"bb_lower_20_2\"] = bb_low\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53570e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# MACRO PANEL (Yahoo Finance)\n",
    "#   Daily Adj Close, ffilled. Same logic as your stock pipeline.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def fetch_macro_panel(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    print(f\"[INFO] Fetching macro tickers from Yahoo: {MACRO_TICKERS}\")\n",
    "    data = yf.download(\n",
    "        MACRO_TICKERS,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        auto_adjust=False,\n",
    "        progress=False,\n",
    "        group_by=\"ticker\",\n",
    "    )\n",
    "\n",
    "    macro = pd.DataFrame()\n",
    "\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        for tkr in MACRO_TICKERS:\n",
    "            if tkr not in data.columns.levels[0]:\n",
    "                print(f\"[WARN] Macro ticker {tkr} missing from download; skipping.\")\n",
    "                continue\n",
    "            s = data[tkr][\"Adj Close\"]\n",
    "            if s.isna().all():\n",
    "                print(f\"[WARN] Macro ticker {tkr} is all-NaN; dropping.\")\n",
    "                continue\n",
    "            macro[tkr] = s\n",
    "    else:\n",
    "        # single-ticker edge case\n",
    "        s = data.get(\"Adj Close\")\n",
    "        if s is not None and not s.isna().all():\n",
    "            macro[MACRO_TICKERS[0]] = s\n",
    "\n",
    "    macro.index.name = \"date\"\n",
    "    macro = macro.sort_index().ffill()\n",
    "    # just in case: drop any columns that are entirely NaN\n",
    "    macro = macro.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    print(f\"[INFO] Macro panel shape: {macro.shape}\")\n",
    "    return macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80902cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# CO2 LOADERS (Refinitiv Excel)\n",
    "#   Handle slightly different layouts for each market.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def load_co2_series(market: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with:\n",
    "      index = datetime (sorted ascending),\n",
    "      column 'price' = closing/spot price.\n",
    "    Sliced to [START_DATE, END_DATE] and drops non-positive/NaN prices.\n",
    "    \"\"\"\n",
    "    path = DATA_DIR / CO2_FILES[market]\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{path} not found\")\n",
    "\n",
    "    print(f\"[INFO] Loading {market} from {path}\")\n",
    "\n",
    "    if market == \"Shanghai\":\n",
    "        # Row 0: 日期 | SAXSHEA (TRDPRC_1)\n",
    "        # Row 1: NaN | 收盘价\n",
    "        # Row 2+: 2025年11月26日 | 54.01, etc.\n",
    "        raw = pd.read_excel(path, header=None)\n",
    "        df = raw.iloc[2:, :2].copy()\n",
    "        df.columns = [\"date\", \"price\"]\n",
    "\n",
    "        # Chinese date format: 2025年11月26日\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\", errors=\"coerce\")\n",
    "        df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"date\", \"price\"])\n",
    "        df = df.set_index(\"date\").sort_index()\n",
    "\n",
    "    elif market in {\"Beijing\", \"Japan\", \"EUA_EEX\"}:\n",
    "        # Structure:\n",
    "        # ...\n",
    "        # row_k: '交易日期' | 'Trade Price'\n",
    "        # rows k+1+: date | price\n",
    "        raw = pd.read_excel(path, header=None)\n",
    "        header_idx = raw.index[raw.iloc[:, 0] == \"交易日期\"][0]\n",
    "        df = raw.iloc[header_idx + 1 :, :2].copy()\n",
    "        df.columns = [\"date\", \"price\"]\n",
    "\n",
    "    elif market == \"EUA_ICE\":\n",
    "        # Structure:\n",
    "        # row31: '交易日期' | '收盘' | '净值' | ...\n",
    "        # rows 32+: full OHLC + extras\n",
    "        raw = pd.read_excel(path, header=None)\n",
    "        header_idx = raw.index[raw.iloc[:, 0] == \"交易日期\"][0]\n",
    "        df = raw.iloc[header_idx + 1 :, :].copy()\n",
    "        cols = raw.iloc[header_idx].tolist()\n",
    "        df.columns = cols\n",
    "        df = df.rename(columns={\"交易日期\": \"date\", \"收盘\": \"price\"})\n",
    "        df = df[[\"date\", \"price\"]]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown market: {market}\")\n",
    "\n",
    "    # Parse & clean\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\", \"price\"])\n",
    "    df = df.set_index(\"date\").sort_index()\n",
    "\n",
    "    # Project slice\n",
    "    df = df.loc[START_DATE:END_DATE]\n",
    "\n",
    "    # Skip missing / illiquid days: price <= 0 or NaNs already dropped\n",
    "    df = df[df[\"price\"] > 0]\n",
    "\n",
    "    print(f\"[INFO] {market} price series shape after slice: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa81f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_by_corr(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    candidate_cols,\n",
    "    k: int = 5,\n",
    "    min_abs_corr: float = 0.05,\n",
    "):\n",
    "    cols = [c for c in candidate_cols if c in df.columns]\n",
    "    if not cols:\n",
    "        return [], pd.Series(dtype=float)\n",
    "\n",
    "    corr_full = df[cols + [target_col]].corr()[target_col].drop(target_col)\n",
    "    corr_full = corr_full.dropna()\n",
    "    if corr_full.empty:\n",
    "        return [], corr_full\n",
    "\n",
    "    # Apply threshold first\n",
    "    corr_sorted = corr_full.abs().sort_values(ascending=False)\n",
    "    corr_filtered = corr_sorted[corr_sorted >= min_abs_corr]\n",
    "\n",
    "    # Fallback: if nothing passes the threshold, just take top-k by abs corr\n",
    "    if corr_filtered.empty:\n",
    "        corr_filtered = corr_sorted\n",
    "\n",
    "    top = list(corr_filtered.head(k).index)\n",
    "    return top, corr_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f783f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# ROLLING WINDOWS: 10-day, 90% train / 10% test\n",
    "#   Mirrors your sliding-window logic but in Keras/XGB.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def rolling_windows(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols,\n",
    "    target_col: str,\n",
    "    window_size: int = 10,\n",
    "    test_ratio: float = 0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Yield rolling windows:\n",
    "      - window_size rows each\n",
    "      - within each window: first 90% for train, last 10% for test\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    for start in range(0, n - window_size + 1):\n",
    "        end = start + window_size\n",
    "        win = df.iloc[start:end]\n",
    "        X_all = win[feature_cols].values\n",
    "        y_all = win[target_col].values\n",
    "        split = int(window_size * (1 - test_ratio))\n",
    "        if split < 1 or split >= window_size:\n",
    "            continue\n",
    "        X_tr, X_te = X_all[:split], X_all[split:]\n",
    "        y_tr, y_te = y_all[:split], y_all[split:]\n",
    "        yield win.index[0], win.index[-1], X_tr, y_tr, X_te, y_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e042fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# MODELS\n",
    "#   - Naive: last train y\n",
    "#   - XGBoost (benchmark ML model)\n",
    "#   - Keras MLP\n",
    "#   - Keras GRU (seq_len=1 for now)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def build_mlp(\n",
    "    input_dim: int,\n",
    "    learning_rate: float = 1e-3,\n",
    "    hidden_units = (64, 32),\n",
    "    dropout: float = 0.1,\n",
    "):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    for h in hidden_units:\n",
    "        model.add(layers.Dense(h, activation=\"relu\"))\n",
    "        if dropout > 0:\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"mse\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_gru(\n",
    "    input_dim: int,\n",
    "    learning_rate: float = 1e-3,\n",
    "    hidden_units: int = 32,\n",
    "    dropout: float = 0.1,\n",
    "):\n",
    "    model = keras.Sequential()\n",
    "    # seq_len=1 for now; later you can upgrade to true sequence inputs\n",
    "    model.add(layers.Input(shape=(1, input_dim)))\n",
    "    model.add(\n",
    "        layers.GRU(\n",
    "            hidden_units,\n",
    "            activation=\"tanh\",\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=0.0,\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"mse\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80424281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# MAIN EVALUATION PER MARKET\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def eval_models_for_market(\n",
    "    market: str,\n",
    "    macro_panel: pd.DataFrame,\n",
    "    window_size: int = 10,\n",
    "    test_ratio: float = 0.1,\n",
    "    n_top_tech: int = 5,\n",
    "    n_top_macro: int = 5,\n",
    "    mlp_config=None,\n",
    "    gru_config=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full pipeline for a single market:\n",
    "      1) Load CO2 prices (2022-2024).\n",
    "      2) Compute technical indicators.\n",
    "      3) Join macro panel (ffill on that market's dates).\n",
    "      4) Define target y = next-day price.\n",
    "      5) Select top-k tech + macro features by |corr(y)|.\n",
    "      6) Run rolling windows with naive, XGB, MLP, GRU.\n",
    "      7) Return metrics + feature list + correlations.\n",
    "    \"\"\"\n",
    "    if mlp_config is None:\n",
    "        mlp_config = DEFAULT_MLP_CONFIG\n",
    "    if gru_config is None:\n",
    "        gru_config = DEFAULT_GRU_CONFIG\n",
    "\n",
    "    # 1) Load price\n",
    "    price_df = load_co2_series(market)\n",
    "    if price_df.empty:\n",
    "        raise ValueError(f\"No data for {market} in {START_DATE}-{END_DATE}\")\n",
    "\n",
    "    # 2) Technicals\n",
    "    tech = compute_technical_features(price_df[\"price\"])\n",
    "\n",
    "    # 3) Macro join: align on this market's dates; we don't create new days\n",
    "    macro = macro_panel.reindex(price_df.index).ffill()\n",
    "\n",
    "    # Combine everything\n",
    "    df = pd.concat([price_df, tech, macro], axis=1)\n",
    "\n",
    "    # 4) Target: next-day price\n",
    "    df[\"y\"] = df[\"price\"].shift(-1)\n",
    "    df = df[df[\"y\"].notna()]\n",
    "\n",
    "    # Candidate feature sets\n",
    "    tech_cols = [c for c in tech.columns if c in df.columns]\n",
    "    macro_cols = [c for c in macro.columns if c in df.columns]\n",
    "\n",
    "    # 5) Feature selection\n",
    "    tech_keep, tech_corr = select_top_by_corr(\n",
    "        df, \"y\", tech_cols, k=n_top_tech\n",
    "    )\n",
    "    macro_keep, macro_corr = select_top_by_corr(\n",
    "        df, \"y\", macro_cols, k=n_top_macro\n",
    "    )\n",
    "\n",
    "    feature_cols = tech_keep + macro_keep\n",
    "\n",
    "    if not feature_cols:\n",
    "        print(f\"[WARN] {market}: no features passed correlation filter; \"\n",
    "            f\"using all tech + macro with non-NaN instead.\")\n",
    "        feature_cols = tech_cols + macro_cols\n",
    "\n",
    "    df_model = df[feature_cols + [\"y\"]].dropna()\n",
    "\n",
    "    print(f\"[INFO] {market} using features:\")\n",
    "    print(\"       TECH:\", tech_keep)\n",
    "    print(\"       MACRO:\", macro_keep)\n",
    "    print(f\"[INFO] {market} model dataset shape: {df_model.shape}\")\n",
    "\n",
    "    # Rolling evaluation\n",
    "    y_true_all = {name: [] for name in [\"naive\", \"xgb\", \"mlp\", \"gru\"]}\n",
    "    y_pred_all = {name: [] for name in [\"naive\", \"xgb\", \"mlp\", \"gru\"]}\n",
    "\n",
    "    for start_date, end_date, X_tr, y_tr, X_te, y_te in rolling_windows(\n",
    "        df_model, feature_cols, \"y\", window_size, test_ratio\n",
    "    ):\n",
    "        if len(y_tr) < 5 or len(y_te) == 0:\n",
    "            continue\n",
    "\n",
    "        # Scale per-window to avoid leakage\n",
    "        sx = StandardScaler().fit(X_tr)\n",
    "        sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
    "\n",
    "        X_tr_s = sx.transform(X_tr)\n",
    "        X_te_s = sx.transform(X_te)\n",
    "        y_tr_s = sy.transform(y_tr.reshape(-1, 1)).ravel()\n",
    "        y_te_s = sy.transform(y_te.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # 1) Naive: last train y\n",
    "        naive_pred_s = np.full_like(y_te_s, fill_value=y_tr_s[-1])\n",
    "\n",
    "        # 2) XGBoost benchmark\n",
    "        xgb = XGBRegressor(\n",
    "            max_depth=3,\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=42,\n",
    "        )\n",
    "        xgb.fit(X_tr_s, y_tr_s)\n",
    "        xgb_pred_s = xgb.predict(X_te_s)\n",
    "\n",
    "        # 3) Keras MLP\n",
    "        mlp = build_mlp(\n",
    "            input_dim=X_tr_s.shape[1],\n",
    "            learning_rate=mlp_config[\"learning_rate\"],\n",
    "            hidden_units=mlp_config[\"hidden_units\"],\n",
    "            dropout=mlp_config[\"dropout\"],\n",
    "        )\n",
    "        mlp.fit(\n",
    "            X_tr_s,\n",
    "            y_tr_s,\n",
    "            epochs=mlp_config[\"epochs\"],\n",
    "            batch_size=mlp_config[\"batch_size\"],\n",
    "            verbose=0,\n",
    "        )\n",
    "        mlp_pred_s = mlp.predict(X_te_s, verbose=0).ravel()\n",
    "\n",
    "        # 4) Keras GRU (seq_len=1)\n",
    "        X_tr_seq = X_tr_s[:, None, :]\n",
    "        X_te_seq = X_te_s[:, None, :]\n",
    "        gru = build_gru(\n",
    "            input_dim=X_tr_s.shape[1],\n",
    "            learning_rate=gru_config[\"learning_rate\"],\n",
    "            hidden_units=gru_config[\"hidden_units\"],\n",
    "            dropout=gru_config[\"dropout\"],\n",
    "        )\n",
    "        gru.fit(\n",
    "            X_tr_seq,\n",
    "            y_tr_s,\n",
    "            epochs=gru_config[\"epochs\"],\n",
    "            batch_size=gru_config[\"batch_size\"],\n",
    "            verbose=0,\n",
    "        )\n",
    "        gru_pred_s = gru.predict(X_te_seq, verbose=0).ravel()\n",
    "\n",
    "        # Inverse-transform all predictions back to price scale\n",
    "        for name, pred_s in [\n",
    "            (\"naive\", naive_pred_s),\n",
    "            (\"xgb\", xgb_pred_s),\n",
    "            (\"mlp\", mlp_pred_s),\n",
    "            (\"gru\", gru_pred_s),\n",
    "        ]:\n",
    "            y_pred = sy.inverse_transform(pred_s.reshape(-1, 1)).ravel()\n",
    "            y_true = sy.inverse_transform(y_te_s.reshape(-1, 1)).ravel()\n",
    "            y_true_all[name].extend(y_true.tolist())\n",
    "            y_pred_all[name].extend(y_pred.tolist())\n",
    "\n",
    "    # Aggregate metrics across all windows\n",
    "    metrics = {}\n",
    "    for name in y_true_all:\n",
    "        if len(y_true_all[name]) == 0:\n",
    "            continue\n",
    "        yt = np.array(y_true_all[name])\n",
    "        yp = np.array(y_pred_all[name])\n",
    "        metrics[name] = {\n",
    "            \"RMSE\": float(np.sqrt(mean_squared_error(yt, yp))),\n",
    "            \"MAPE\": float(mean_absolute_percentage_error(yt, yp)),\n",
    "            \"R2\": float(r2_score(yt, yp)),\n",
    "            \"n_points\": int(len(yt)),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"market\": market,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"tech_corr\": tech_corr,\n",
    "        \"macro_corr\": macro_corr,\n",
    "        \"metrics\": metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7662617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching macro tickers from Yahoo: ['^GSPC', '^IXIC', '000001.SS', '399001.SZ', '^N225', '^STOXX50E', 'CL=F', 'BZ=F', 'NG=F', 'GC=F', 'SI=F', 'HG=F', 'DX-Y.NYB']\n",
      "[INFO] Macro panel shape: (779, 13)\n",
      "================================================================================\n",
      "[RUN] Shanghai\n",
      "[INFO] Loading Shanghai from data/Shanghai Spot - 2020.8.1 - 2025.11.28.xlsx\n",
      "[ERROR] Shanghai: 'date'\n",
      "================================================================================\n",
      "[RUN] Beijing\n",
      "[INFO] Loading Beijing from data/Beijing - 2020.8.1 - 2025.11.28.xlsx\n",
      "[INFO] Beijing price series shape after slice: (193, 1)\n",
      "[INFO] Beijing using features:\n",
      "       TECH: ['sma_5', 'sma_20', 'sma_60', 'bb_lower_20_2', 'vol_20d']\n",
      "       MACRO: ['SI=F', '^GSPC', 'GC=F', '^IXIC', '^STOXX50E']\n",
      "[INFO] Beijing model dataset shape: (133, 11)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x317132660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3206227a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[RESULT] Beijing metrics:\n",
      "  naive  | RMSE=10.5396  MAPE=0.0642  R2=0.0410  (n=124)\n",
      "  xgb    | RMSE=9.1414  MAPE=0.0607  R2=0.2786  (n=124)\n",
      "  mlp    | RMSE=10.9079  MAPE=0.0684  R2=-0.0272  (n=124)\n",
      "  gru    | RMSE=8.3605  MAPE=0.0578  R2=0.3966  (n=124)\n",
      "================================================================================\n",
      "[RUN] Japan\n",
      "[INFO] Loading Japan from data/Japan - 01-Oct-2023 - 28-Nov-2025.xlsx\n",
      "[INFO] Japan price series shape after slice: (131, 1)\n",
      "[INFO] Japan using features:\n",
      "       TECH: ['sma_5', 'bb_upper_20_2', 'sma_20', 'macd_signal', 'sma_60']\n",
      "       MACRO: ['000001.SS', 'BZ=F', '399001.SZ', 'NG=F', 'CL=F']\n",
      "[INFO] Japan model dataset shape: (71, 11)\n",
      "[RESULT] Japan metrics:\n",
      "  naive  | RMSE=67.3247  MAPE=0.0227  R2=0.9634  (n=62)\n",
      "  xgb    | RMSE=73.6987  MAPE=0.0266  R2=0.9562  (n=62)\n",
      "  mlp    | RMSE=71.5529  MAPE=0.0271  R2=0.9587  (n=62)\n",
      "  gru    | RMSE=69.0417  MAPE=0.0268  R2=0.9616  (n=62)\n",
      "================================================================================\n",
      "[RUN] EUA_ICE\n",
      "[INFO] Loading EUA_ICE from data/EUA - 01-Sep-2020 - 24-Nov-2025.xlsx\n",
      "[INFO] EUA_ICE price series shape after slice: (770, 1)\n",
      "[INFO] EUA_ICE using features:\n",
      "       TECH: ['sma_5', 'sma_20', 'bb_lower_20_2', 'bb_upper_20_2', 'sma_60']\n",
      "       MACRO: ['^N225', '^GSPC', '^IXIC', 'GC=F', '^STOXX50E']\n",
      "[INFO] EUA_ICE model dataset shape: (710, 11)\n",
      "[RESULT] EUA_ICE metrics:\n",
      "  naive  | RMSE=1.8901  MAPE=0.0173  R2=0.9809  (n=701)\n",
      "  xgb    | RMSE=2.4475  MAPE=0.0221  R2=0.9679  (n=701)\n",
      "  mlp    | RMSE=2.6021  MAPE=0.0237  R2=0.9637  (n=701)\n",
      "  gru    | RMSE=2.6186  MAPE=0.0239  R2=0.9633  (n=701)\n",
      "================================================================================\n",
      "[RUN] EUA_EEX\n",
      "[INFO] Loading EUA_EEX from data/EEX-EUSP4-SPOT - 01Jan2021 - 28Nov 2025.xlsx\n",
      "[INFO] EUA_EEX price series shape after slice: (766, 1)\n",
      "[INFO] EUA_EEX using features:\n",
      "       TECH: ['sma_5', 'sma_20', 'bb_lower_20_2', 'bb_upper_20_2', 'sma_60']\n",
      "       MACRO: ['^N225', '399001.SZ', '^GSPC', '^IXIC', 'GC=F']\n",
      "[INFO] EUA_EEX model dataset shape: (706, 11)\n",
      "[RESULT] EUA_EEX metrics:\n",
      "  naive  | RMSE=1.7778  MAPE=0.0179  R2=0.9703  (n=697)\n",
      "  xgb    | RMSE=2.2693  MAPE=0.0228  R2=0.9517  (n=697)\n",
      "  mlp    | RMSE=2.6021  MAPE=0.0248  R2=0.9365  (n=697)\n",
      "  gru    | RMSE=2.4622  MAPE=0.0250  R2=0.9431  (n=697)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# MAIN (for quick proof-of-concept run)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Macro panel once\n",
    "    macro_panel = fetch_macro_panel(START_DATE, END_DATE)\n",
    "\n",
    "    # 2) Run over all markets\n",
    "    all_results = {}\n",
    "    for mkt in CO2_FILES:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"[RUN] {mkt}\")\n",
    "        try:\n",
    "            res = eval_models_for_market(mkt, macro_panel)\n",
    "            all_results[mkt] = res\n",
    "            print(f\"[RESULT] {mkt} metrics:\")\n",
    "            for model_name, m in res[\"metrics\"].items():\n",
    "                print(\n",
    "                    f\"  {model_name:6s} | \"\n",
    "                    f\"RMSE={m['RMSE']:.4f}  \"\n",
    "                    f\"MAPE={m['MAPE']:.4f}  \"\n",
    "                    f\"R2={m['R2']:.4f}  \"\n",
    "                    f\"(n={m['n_points']})\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {mkt}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_co2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
